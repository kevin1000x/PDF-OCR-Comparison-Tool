"""
æ¨¡å‹ä¼˜åŒ–æ¨¡å— - è‡ªåŠ¨å‚æ•°é€‰æ‹©å’Œæ€§èƒ½è°ƒä¼˜
=========================================

æ ¹æ®GPUé…ç½®å’Œæ–‡æ¡£ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€ä½³OCRå‚æ•°
"""

import os
import sys
import time
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class GPUTier(Enum):
    """GPUæ€§èƒ½ç­‰çº§"""
    HIGH = "high"       # RTX 4080+, 16GB+
    MEDIUM = "medium"   # RTX 3060-4070, 8-12GB
    LOW = "low"         # GTX 1060-RTX 2060, 6-8GB
    CPU = "cpu"         # æ— GPUæˆ–GPUä¸å¯ç”¨


class DocumentComplexity(Enum):
    """æ–‡æ¡£å¤æ‚åº¦"""
    SIMPLE = "simple"       # çº¯æ–‡æœ¬ï¼Œæ¸…æ™°
    MEDIUM = "medium"       # å›¾è¡¨æ··æ’ï¼Œä¸€èˆ¬æ¸…æ™°åº¦
    COMPLEX = "complex"     # æ‰‹å†™+å°åˆ·æ··åˆï¼Œæ¨¡ç³Š


@dataclass
class OptimalConfig:
    """æœ€ä¼˜é…ç½®"""
    engine: str
    dpi: int
    image_size: int
    base_size: int
    batch_size: int
    use_gpu: bool
    use_fp16: bool
    estimated_speed: float  # é¢„ä¼°æ¯é¡µå¤„ç†æ—¶é—´(ç§’)


class ModelOptimizer:
    """æ¨¡å‹ä¼˜åŒ–å™¨ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®"""
    
    def __init__(self):
        self.gpu_info = self._detect_gpu()
        self.gpu_tier = self._classify_gpu()
        
        # é…ç½®é¢„è®¾
        self.presets = self._load_presets()
    
    def _detect_gpu(self) -> Dict:
        """æ£€æµ‹GPUä¿¡æ¯"""
        try:
            import torch
            
            if not torch.cuda.is_available():
                return {'available': False}
            
            device = torch.cuda.get_device_properties(0)
            total_memory = device.total_memory / 1024**3  # GB
            
            return {
                'available': True,
                'name': device.name,
                'total_memory': total_memory,
                'compute_capability': f"{device.major}.{device.minor}",
                'multi_processor_count': device.multi_processor_count
            }
        except Exception as e:
            logger.warning(f"GPU detection failed: {e}")
            return {'available': False}
    
    def _classify_gpu(self) -> GPUTier:
        """åˆ†ç±»GPUæ€§èƒ½ç­‰çº§"""
        if not self.gpu_info.get('available'):
            return GPUTier.CPU
        
        memory = self.gpu_info.get('total_memory', 0)
        name = self.gpu_info.get('name', '').lower()
        
        # æ ¹æ®æ˜¾å­˜å’Œå‹å·åˆ†ç±»
        if memory >= 16:
            return GPUTier.HIGH
        elif memory >= 10:
            return GPUTier.MEDIUM
        elif memory >= 6:
            return GPUTier.LOW
        else:
            return GPUTier.CPU
    
    def _load_presets(self) -> Dict[GPUTier, Dict]:
        """åŠ è½½é¢„è®¾é…ç½®"""
        return {
            GPUTier.HIGH: {
                'deepseek': {
                    'dpi': 200,
                    'image_size': 1024,
                    'base_size': 1536,
                    'batch_size': 4,
                    'use_fp16': True,
                    'estimated_speed': 5.0
                },
                'paddle': {
                    'dpi': 200,
                    'batch_size': 16,
                    'use_fp16': True,
                    'estimated_speed': 2.0
                }
            },
            GPUTier.MEDIUM: {
                'deepseek': {
                    'dpi': 150,
                    'image_size': 768,
                    'base_size': 1024,
                    'batch_size': 2,
                    'use_fp16': True,
                    'estimated_speed': 10.0
                },
                'paddle': {
                    'dpi': 150,
                    'batch_size': 8,
                    'use_fp16': True,
                    'estimated_speed': 3.0
                }
            },
            GPUTier.LOW: {
                'deepseek': {
                    'dpi': 100,
                    'image_size': 512,
                    'base_size': 768,
                    'batch_size': 1,
                    'use_fp16': True,
                    'estimated_speed': 20.0
                },
                'paddle': {
                    'dpi': 150,
                    'batch_size': 4,
                    'use_fp16': True,
                    'estimated_speed': 5.0
                }
            },
            GPUTier.CPU: {
                'deepseek': None,  # CPUä¸æ¨èDeepSeek
                'paddle': {
                    'dpi': 100,
                    'batch_size': 1,
                    'use_fp16': False,
                    'estimated_speed': 15.0
                }
            }
        }
    
    def get_optimal_config(
        self,
        prefer_accuracy: bool = True,
        document_complexity: DocumentComplexity = DocumentComplexity.MEDIUM,
        available_time: Optional[float] = None,
        total_pages: Optional[int] = None
    ) -> OptimalConfig:
        """
        è·å–æœ€ä¼˜é…ç½®
        
        Args:
            prefer_accuracy: æ˜¯å¦ä¼˜å…ˆç²¾åº¦ï¼ˆå¦åˆ™ä¼˜å…ˆé€Ÿåº¦ï¼‰
            document_complexity: æ–‡æ¡£å¤æ‚åº¦
            available_time: å¯ç”¨æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œç”¨äºé€‰æ‹©åˆé€‚çš„é…ç½®
            total_pages: æ€»é¡µæ•°ï¼Œç”¨äºä¼°ç®—æ˜¯å¦å¯è¡Œ
            
        Returns:
            æœ€ä¼˜é…ç½®
        """
        presets = self.presets[self.gpu_tier]
        
        # é€‰æ‹©å¼•æ“
        if self.gpu_tier == GPUTier.CPU:
            engine = 'paddle'
        elif prefer_accuracy:
            engine = 'deepseek' if presets.get('deepseek') else 'paddle'
        else:
            engine = 'paddle'
        
        preset = presets[engine]
        if preset is None:
            # é™çº§åˆ°paddle
            engine = 'paddle'
            preset = presets['paddle']
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´
        dpi = preset['dpi']
        if document_complexity == DocumentComplexity.COMPLEX:
            dpi = min(dpi + 50, 300)
        elif document_complexity == DocumentComplexity.SIMPLE:
            dpi = max(dpi - 50, 72)
        
        # å¦‚æœæœ‰æ—¶é—´é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦å¯è¡Œ
        estimated_speed = preset['estimated_speed']
        if available_time and total_pages:
            required_time = total_pages * estimated_speed
            if required_time > available_time:
                # éœ€è¦æ›´å¿«çš„é…ç½®
                if engine == 'deepseek':
                    engine = 'paddle'
                    preset = presets['paddle']
                    estimated_speed = preset['estimated_speed']
                dpi = max(dpi - 50, 72)
        
        return OptimalConfig(
            engine=engine,
            dpi=dpi,
            image_size=preset.get('image_size', 768),
            base_size=preset.get('base_size', 1024),
            batch_size=preset.get('batch_size', 1),
            use_gpu=self.gpu_tier != GPUTier.CPU,
            use_fp16=preset.get('use_fp16', True),
            estimated_speed=estimated_speed
        )
    
    def estimate_processing_time(
        self,
        total_pages: int,
        config: Optional[OptimalConfig] = None
    ) -> Dict:
        """
        ä¼°ç®—å¤„ç†æ—¶é—´
        
        Args:
            total_pages: æ€»é¡µæ•°
            config: é…ç½®ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            
        Returns:
            æ—¶é—´ä¼°ç®—ä¿¡æ¯
        """
        if config is None:
            config = self.get_optimal_config()
        
        time_per_page = config.estimated_speed
        total_time = total_pages * time_per_page
        
        return {
            'total_pages': total_pages,
            'time_per_page': time_per_page,
            'total_time_seconds': total_time,
            'total_time_minutes': total_time / 60,
            'total_time_hours': total_time / 3600,
            'formatted': self._format_time(total_time)
        }
    
    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´"""
        if seconds < 60:
            return f"{seconds:.0f}ç§’"
        elif seconds < 3600:
            return f"{seconds/60:.1f}åˆ†é’Ÿ"
        else:
            return f"{seconds/3600:.1f}å°æ—¶"
    
    def run_benchmark(self, test_image_path: str = None) -> Dict:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            test_image_path: æµ‹è¯•å›¾åƒè·¯å¾„
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        results = {
            'gpu_info': self.gpu_info,
            'gpu_tier': self.gpu_tier.value,
            'tests': {}
        }
        
        # æµ‹è¯•ä¸åŒé…ç½®
        configs_to_test = [
            ('deepseek_high', {'engine': 'deepseek', 'dpi': 200}),
            ('deepseek_low', {'engine': 'deepseek', 'dpi': 100}),
            ('paddle', {'engine': 'paddle', 'dpi': 150})
        ]
        
        if self.gpu_tier != GPUTier.CPU and test_image_path:
            for name, config in configs_to_test:
                try:
                    speed = self._benchmark_config(test_image_path, config)
                    results['tests'][name] = {
                        'config': config,
                        'speed': speed,
                        'status': 'success'
                    }
                except Exception as e:
                    results['tests'][name] = {
                        'config': config,
                        'error': str(e),
                        'status': 'failed'
                    }
        
        return results
    
    def _benchmark_config(self, image_path: str, config: Dict) -> float:
        """æµ‹è¯•å•ä¸ªé…ç½®çš„é€Ÿåº¦"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦åŠ è½½æ¨¡å‹å¹¶è¿è¡Œ
        # è¿™é‡Œè¿”å›é¢„ä¼°å€¼
        if config['engine'] == 'deepseek':
            return self.presets[self.gpu_tier].get('deepseek', {}).get('estimated_speed', 15)
        else:
            return self.presets[self.gpu_tier].get('paddle', {}).get('estimated_speed', 5)
    
    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform
        import torch
        
        info = {
            'os': platform.system(),
            'os_version': platform.version(),
            'python_version': platform.python_version(),
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
        }
        
        if torch.cuda.is_available():
            info.update({
                'cuda_version': torch.version.cuda,
                'cudnn_version': torch.backends.cudnn.version(),
                'gpu_count': torch.cuda.device_count(),
                'gpu_name': torch.cuda.get_device_name(0),
                'gpu_memory': f"{self.gpu_info.get('total_memory', 0):.1f} GB"
            })
        
        return info
    
    def recommend_action(self) -> str:
        """ç»™å‡ºä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if self.gpu_tier == GPUTier.CPU:
            recommendations.append("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå»ºè®®å®‰è£…CUDAå’Œæ”¯æŒGPUçš„PyTorch")
            recommendations.append("ğŸ’¡ ä½¿ç”¨PaddleOCRä½œä¸ºOCRå¼•æ“ï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰")
        elif self.gpu_tier == GPUTier.LOW:
            recommendations.append("âš ï¸ GPUæ˜¾å­˜è¾ƒå°ï¼Œå»ºè®®ï¼š")
            recommendations.append("  - å…³é—­åå°åº”ç”¨é‡Šæ”¾æ˜¾å­˜")
            recommendations.append("  - ä½¿ç”¨DPI=100é™ä½å†…å­˜å ç”¨")
            recommendations.append("  - è€ƒè™‘ä½¿ç”¨PaddleOCRæ›¿ä»£DeepSeek-OCR2")
        elif self.gpu_tier == GPUTier.MEDIUM:
            recommendations.append("âœ… GPUé…ç½®é€‚ä¸­ï¼Œå»ºè®®ï¼š")
            recommendations.append("  - ä½¿ç”¨DPI=150å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦")
            recommendations.append("  - å…³é—­ä¸å¿…è¦çš„åå°åº”ç”¨")
        else:
            recommendations.append("âœ… GPUé…ç½®ä¼˜ç§€ï¼Œå¯ä»¥ä½¿ç”¨æœ€é«˜è´¨é‡è®¾ç½®")
            recommendations.append("  - ä½¿ç”¨DPI=200è·å¾—æœ€ä½³ç²¾åº¦")
            recommendations.append("  - å¯ä»¥å¯ç”¨æ‰¹é‡å¤„ç†")
        
        return "\n".join(recommendations)


def get_auto_config() -> OptimalConfig:
    """è·å–è‡ªåŠ¨ä¼˜åŒ–é…ç½®"""
    optimizer = ModelOptimizer()
    return optimizer.get_optimal_config()


def print_system_report():
    """æ‰“å°ç³»ç»ŸæŠ¥å‘Š"""
    optimizer = ModelOptimizer()
    
    print("=" * 60)
    print("ç³»ç»Ÿé…ç½®æŠ¥å‘Š")
    print("=" * 60)
    
    info = optimizer.get_system_info()
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    print("\n" + "-" * 60)
    print(f"GPUç­‰çº§: {optimizer.gpu_tier.value}")
    print("-" * 60)
    
    config = optimizer.get_optimal_config()
    print("\næ¨èé…ç½®:")
    print(f"  OCRå¼•æ“: {config.engine}")
    print(f"  DPI: {config.dpi}")
    print(f"  ä½¿ç”¨GPU: {config.use_gpu}")
    print(f"  ä½¿ç”¨FP16: {config.use_fp16}")
    print(f"  é¢„ä¼°é€Ÿåº¦: {config.estimated_speed}ç§’/é¡µ")
    
    print("\n" + "-" * 60)
    print("ä¼˜åŒ–å»ºè®®:")
    print(optimizer.recommend_action())
    print("=" * 60)


if __name__ == "__main__":
    print_system_report()
